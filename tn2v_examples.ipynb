{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8f34743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from os.path import expanduser\n",
    "home = expanduser(\"~\")\n",
    "\n",
    "import matplotlib\n",
    "import scipy\n",
    "import time\n",
    "\n",
    "import tn2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a90563",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d84a6b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def circles(c=1,circle_density=20,mr=None,noise=0.05):\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    circles = c\n",
    "    meta_angles = np.linspace(0,2*np.pi,circles+1)[:circles]\n",
    "\n",
    "    if mr is None:\n",
    "        meta_radius = 0.51 + .08*(c-4)\n",
    "    else:\n",
    "        meta_radius = mr\n",
    "    inner_radius = 0.20\n",
    "\n",
    "    for i in range(circles):\n",
    "        angle_noise = np.random.normal(loc=0,scale=2*np.pi/circle_density*noise,size=circle_density)\n",
    "        inner_angles = np.linspace(0,2*np.pi,circle_density+1)[:circle_density] + angle_noise\n",
    "\n",
    "        radius_noise = np.random.normal(loc=0,scale=inner_radius*noise,size=circle_density)\n",
    "        inner_radii = np.full(circle_density,inner_radius) + radius_noise\n",
    "        for j in range(circle_density):\n",
    "            a = meta_radius*np.cos(meta_angles[i]) + inner_radii[j]*np.cos(inner_angles[j])\n",
    "            b = meta_radius*np.sin(meta_angles[i]) + inner_radii[j]*np.sin(inner_angles[j])\n",
    "\n",
    "            x.append(a)\n",
    "            y.append(b)\n",
    "\n",
    "    return pd.DataFrame([[x[i],y[i]] for i in range(len(x))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bcb4705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def torus(outer_rad=1.0,inner_rad=0.5,spreader_coeff=0.175,noise=0.05):\n",
    "\n",
    "    cycle1_min_rad = outer_rad - inner_rad\n",
    "    cycle1_max_rad = outer_rad + inner_rad\n",
    "\n",
    "    cycle2_circ = np.pi*2*inner_rad\n",
    "    cycle1_min_circ = np.pi*2*cycle1_min_rad\n",
    "    cycle1_max_circ = np.pi*2*cycle1_max_rad\n",
    "\n",
    "    inner_angle_count = int(cycle2_circ/spreader_coeff)\n",
    "    inner_angle_increment = 2*np.pi/inner_angle_count\n",
    "\n",
    "    _inner_angles = np.linspace(0,2*np.pi,inner_angle_count)\n",
    "\n",
    "    _outer_angles_dict = {}\n",
    "    for i in range(len(_inner_angles)):\n",
    "        angle = _inner_angles[i]\n",
    "        circ = (outer_rad + np.cos(angle)*inner_rad)*2*np.pi\n",
    "        outer_angle_count = int(circ/spreader_coeff)\n",
    "        _outer_angles_dict[i] = np.linspace(0,2*np.pi,outer_angle_count)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    z = []\n",
    "\n",
    "    for i in range(len(_inner_angles)):\n",
    "        inner_angle = _inner_angles[i]\n",
    "        for outer_angle in _outer_angles_dict[i]:\n",
    "            x.append((outer_rad+inner_rad*np.cos(inner_angle))*np.cos(outer_angle))\n",
    "            y.append((outer_rad+inner_rad*np.cos(inner_angle))*np.sin(outer_angle))\n",
    "            z.append(inner_rad*np.sin(inner_angle))\n",
    "    \n",
    "    D = pd.DataFrame([x,y,z]).transpose()\n",
    "    E = pd.DataFrame(np.random.uniform(-noise,noise,D.shape[0]*3).reshape(D.shape[0],3))\n",
    "    \n",
    "    return D+E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebcb604",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f7d5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# this command suppresses all the print statements I have in the code — you probably want to use this unless you're troubleshooting\n",
    "\n",
    "embedding_dimension = 2 # desired embedding dimension\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "LEN = 5000\n",
    "\n",
    "#eta_array = [.1 for i in range(LEN+1)]\n",
    "eta_array = np.linspace(0.005,0.001,LEN+1)\n",
    "for i in range(100):\n",
    "    eta_array[i] = 1.0\n",
    "\n",
    "lambda0 = 1 # for node2vec\n",
    "lambda1 = 192 # for dim1 homology\n",
    "lambda2 = 0 # for dim2 homology\n",
    "\n",
    "L0_array = [lambda0 for i in range(LEN+1)]\n",
    "L1_array = [lambda1*float(LEN/4+(3*i/4))/float(LEN)*int(i>100) for i in range(LEN+1)]\n",
    "L2_array = [lambda2 for i in range(LEN+1)]\n",
    "\n",
    "main_directory = home+'/tn2v_output/'\n",
    "# local directory for saving output\n",
    "\n",
    "if not os.path.isdir(main_directory):\n",
    "    os.mkdir(main_directory)\n",
    "\n",
    "    \n",
    "\n",
    "#### GENERATE INPUT DATA\n",
    "    \n",
    "# circle number\n",
    "cn = 8\n",
    "\n",
    "# circle density\n",
    "cd = 16\n",
    "\n",
    "data = circles(cn,cd) # input data, should be a pd.DataFrame\n",
    "\n",
    "mode = 'pointcloud'\n",
    "# this describes the input type\n",
    "# alternatives are 'correlationmatrix' and 'distancematrix'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### NODE2VEC NBHD GENERATION PARAMETERS\n",
    "\n",
    "r = 1000 # number of walks generated from each vertex in node2vec\n",
    "l = 1000 # length of walks generated from each vertex in node2vec\n",
    "\n",
    "L_array = [l for i in range(LEN+1)]\n",
    "R_array = [r for i in range(LEN+1)]\n",
    "P_array = [0 for i in range(LEN+1)]\n",
    "Q_array = [1 for i in range(LEN+1)]\n",
    "\n",
    "param_array = [{'l':L_array[i],\n",
    "                'r':R_array[i],\n",
    "                'p':P_array[i],\n",
    "                'q':Q_array[i]}\n",
    "               for i in range(LEN+1)]\n",
    "\n",
    "nbhd_regen = None\n",
    "# this determines how often/if the nbhds are regenerated using the variables above\n",
    "# if nbhd_regen is None OR l*r > the size of the data set, no nbhd will be generated,\n",
    "#     and instead the full vector of traversal probability (edge weights or reciprocal distances)\n",
    "#     will be used instead\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### FURTHER ADJUSTMENT\n",
    "\n",
    "mbs_array = [int(data.shape[0]*0.125) for i in range(LEN+1)] # mini-batch size — input is in number of data points, adjust the multiplier to choose by percent\n",
    "\n",
    "grad_old = False\n",
    "\n",
    "lift_array = [.125 for i in range(LEN+1)] # if you want to lift the PDs before matching, assign here\n",
    "\n",
    "gpd = None # if you want to GIVE an artificial target PD, assign it here (as a pd.DataFrame)\n",
    "W1_data = None # if you want to start with a non-random initial state for W1, assign it here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### OUTPUT SETTINGS\n",
    "\n",
    "pointcloud_vf_save = np.arange(LEN+1,step=100) # at what epochs do you want to save .png files of the pointcloud with vector fields denoting gradient movement?\n",
    "\n",
    "pointcloud_data_save = np.arange(LEN+1,step=100) # at what epochs do you want to save .csv files of the current embedding?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "project_name = 's1x8_example/'\n",
    "\n",
    "\n",
    "overwrite = True\n",
    "if os.path.isdir(main_directory+project_name) and not overwrite:\n",
    "    \n",
    "    print('This project directory already exists.')\n",
    "\n",
    "else:\n",
    "\n",
    "    X = tn2v.tn2v(\n",
    "        main_directory=main_directory,\n",
    "        project_name=project_name,\n",
    "        data=data,\n",
    "        mode=mode,\n",
    "        embedding_dimension=embedding_dimension,\n",
    "        param_array=param_array,\n",
    "        l0_array=L0_array,\n",
    "        l1_array=L1_array,\n",
    "        l2_array=L2_array,\n",
    "        eta_array=eta_array,\n",
    "        LEN=LEN,\n",
    "        mbs_array=mbs_array,\n",
    "        lift_array=lift_array,\n",
    "        gpd=gpd,\n",
    "        grad_old=grad_old,\n",
    "        nbhd_regen=nbhd_regen,\n",
    "        pointcloud_data_save=pointcloud_data_save,\n",
    "        pointcloud_vf_save=pointcloud_vf_save,\n",
    "        cpu_gpu='gpu'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22429a46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8b2cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# this command suppresses the many (excessive) print statements I have in the code — you probably want to use this unless you're troubleshooting\n",
    "\n",
    "embedding_dimension = 3 # desired embedding dimension\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### SETUP\n",
    "\n",
    "LEN = 40000\n",
    "\n",
    "eta_array = np.linspace(0.0025,0.000125,LEN+1)\n",
    "\n",
    "# note that here and in the L* arrays below, the first 100 steps are left to full power Node2vec in order to rapidly move points to roughly their proper positions before trying to open up topological features\n",
    "for i in range(100):\n",
    "    eta_array[i] = 1.0\n",
    "\n",
    "lambda0 = 1.0 # for node2vec\n",
    "lambda1 = 256 # for dim1 homology\n",
    "lambda2 = 784 # for dim2 homology\n",
    "\n",
    "L0_array = [lambda0 for i in range(LEN+1)]\n",
    "L1_array = [lambda1*float(LEN/4+(3*i/4))/float(LEN)*int(i>100) for i in range(LEN+1)]\n",
    "L2_array = [lambda2*float(LEN/4+(3*i/4))/float(LEN)*int(i>100) for i in range(LEN+1)]\n",
    "\n",
    "main_directory = home+'/tn2v_output/'\n",
    "# local directory for saving output\n",
    "\n",
    "if not os.path.isdir(main_directory):\n",
    "    os.mkdir(main_directory)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "#### INPUT DATA\n",
    "    \n",
    "data = torus() # input data, should be a pd.DataFrame\n",
    "\n",
    "mode = 'pointcloud'\n",
    "# this describes the input type\n",
    "# alternatives are 'correlationmatrix' and 'distancematrix'\n",
    "\n",
    "alpha = 0.8\n",
    "# the distance-to-adjacency-matrix reciprocal parameter\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### NODE2VEC NBHD GENERATION PARAMETERS\n",
    "\n",
    "r = 1000 # number of walks generated from each vertex in node2vec\n",
    "l = 1000 # length of walks generated from each vertex in node2vec\n",
    "\n",
    "L_array = [l for i in range(LEN+1)]\n",
    "R_array = [r for i in range(LEN+1)]\n",
    "P_array = [0 for i in range(LEN+1)]\n",
    "Q_array = [1 for i in range(LEN+1)]\n",
    "\n",
    "param_array = [{'l':L_array[i],\n",
    "                'r':R_array[i],\n",
    "                'p':P_array[i],\n",
    "                'q':Q_array[i]}\n",
    "               for i in range(LEN+1)]\n",
    "\n",
    "nbhd_regen = None\n",
    "# this determines how often / if the nbhds are regenerated using the variables above\n",
    "# if nbhd_regen is None OR l*r > the size of the data set, no nbhd will be generated;\n",
    "# instead the full vector of traversal probability (edge weights or reciprocal distances) will be used\n",
    "# see the l = infinity remark in the paper for explanation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### FURTHER ADJUSTMENT\n",
    "\n",
    "mbs_array = [int(data.shape[0]*0.0625) for i in range(LEN+1)] # mini-batch size — input is in number of data points, adjust the multiplier to choose by percent\n",
    "\n",
    "lift_array = [0.75 for i in range(LEN+1)] # if you want to lift the PDs before matching, assign here\n",
    "\n",
    "\n",
    "\n",
    "#### OUTPUT SETTINGS\n",
    "\n",
    "pointcloud_vf_save = np.arange(LEN+1,step=100) # at what epochs do you want to save .png files of the pointcloud with vector fields denoting gradient movement?\n",
    "\n",
    "pointcloud_data_save = np.arange(LEN+1,step=100) # at what epochs do you want to save .csv files of the current embedding?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "project_name = 'example_torus/'\n",
    "\n",
    "overwrite = False\n",
    "if os.path.isdir(main_directory+project_name) and not overwrite:\n",
    "    print('This project directory already exists.')\n",
    "\n",
    "else:\n",
    "    X = tn2v.tn2v(\n",
    "        main_directory=main_directory,\n",
    "        project_name=project_name,\n",
    "        data=data,\n",
    "        mode=mode,\n",
    "        embedding_dimension=embedding_dimension,\n",
    "        param_array=param_array,\n",
    "        l0_array=L0_array,\n",
    "        l1_array=L1_array,\n",
    "        l2_array=L2_array,\n",
    "        eta_array=eta_array,\n",
    "        LEN=LEN,\n",
    "        mbs_array=mbs_array,\n",
    "        lift_array=lift_array,\n",
    "        gpd=gpd,\n",
    "        grad_old=grad_old,\n",
    "        nbhd_regen=nbhd_regen,\n",
    "        pointcloud_data_save=pointcloud_data_save,\n",
    "        pointcloud_vf_save=pointcloud_vf_save,\n",
    "        alpha=alpha,\n",
    "        cpu_gpu='gpu'\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
