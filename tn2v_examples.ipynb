{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a40197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from os.path import expanduser\n",
    "home = expanduser(\"~\")\n",
    "\n",
    "import matplotlib\n",
    "import scipy\n",
    "import time\n",
    "\n",
    "import tn2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3251f308",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5713b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def circles(c=1,circle_density=20,mr=None,noise=0.05):\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    circles = c\n",
    "    meta_angles = np.linspace(0,2*np.pi,circles+1)[:circles]\n",
    "\n",
    "    if mr is None:\n",
    "        meta_radius = 0.51 + .08*(c-4)\n",
    "    else:\n",
    "        meta_radius = mr\n",
    "    inner_radius = 0.20\n",
    "\n",
    "    for i in range(circles):\n",
    "        angle_noise = np.random.normal(loc=0,scale=2*np.pi/circle_density*noise,size=circle_density)\n",
    "        inner_angles = np.linspace(0,2*np.pi,circle_density+1)[:circle_density] + angle_noise\n",
    "\n",
    "        radius_noise = np.random.normal(loc=0,scale=inner_radius*noise,size=circle_density)\n",
    "        inner_radii = np.full(circle_density,inner_radius) + radius_noise\n",
    "        for j in range(circle_density):\n",
    "            a = meta_radius*np.cos(meta_angles[i]) + inner_radii[j]*np.cos(inner_angles[j])\n",
    "            b = meta_radius*np.sin(meta_angles[i]) + inner_radii[j]*np.sin(inner_angles[j])\n",
    "\n",
    "            x.append(a)\n",
    "            y.append(b)\n",
    "\n",
    "    return pd.DataFrame([[x[i],y[i]] for i in range(len(x))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2101486f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def torus(outer_rad=1.0,inner_rad=0.5,spreader_coeff=0.175,noise=0.05):\n",
    "\n",
    "    cycle1_min_rad = outer_rad - inner_rad\n",
    "    cycle1_max_rad = outer_rad + inner_rad\n",
    "\n",
    "    cycle2_circ = np.pi*2*inner_rad\n",
    "    cycle1_min_circ = np.pi*2*cycle1_min_rad\n",
    "    cycle1_max_circ = np.pi*2*cycle1_max_rad\n",
    "\n",
    "    inner_angle_count = int(cycle2_circ/spreader_coeff)\n",
    "    inner_angle_increment = 2*np.pi/inner_angle_count\n",
    "\n",
    "    _inner_angles = np.linspace(0,2*np.pi,inner_angle_count)\n",
    "\n",
    "    _outer_angles_dict = {}\n",
    "    for i in range(len(_inner_angles)):\n",
    "        angle = _inner_angles[i]\n",
    "        circ = (outer_rad + np.cos(angle)*inner_rad)*2*np.pi\n",
    "        outer_angle_count = int(circ/spreader_coeff)\n",
    "        _outer_angles_dict[i] = np.linspace(0,2*np.pi,outer_angle_count)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    z = []\n",
    "\n",
    "    for i in range(len(_inner_angles)):\n",
    "        inner_angle = _inner_angles[i]\n",
    "        for outer_angle in _outer_angles_dict[i]:\n",
    "            x.append((outer_rad+inner_rad*np.cos(inner_angle))*np.cos(outer_angle))\n",
    "            y.append((outer_rad+inner_rad*np.cos(inner_angle))*np.sin(outer_angle))\n",
    "            z.append(inner_rad*np.sin(inner_angle))\n",
    "    \n",
    "    D = pd.DataFrame([x,y,z]).transpose()\n",
    "    E = pd.DataFrame(np.random.uniform(-noise,noise,D.shape[0]*3).reshape(D.shape[0],3))\n",
    "    \n",
    "    return D+E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ebee1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b58cbdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cb470a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "main_directory = home+'/tn2v_output/'\n",
    "# local directory for saving output\n",
    "\n",
    "if not os.path.isdir(main_directory):\n",
    "    os.mkdir(main_directory)\n",
    "\n",
    "    \n",
    "\n",
    "project_name = 's1x8_example/'\n",
    "# subfolder for this project\n",
    "\n",
    "be_careful = False\n",
    "# True: if there is already a project directory with this project_name, stop\n",
    "# False: you're running the same experiment again and just want to overwrite\n",
    "\n",
    "\n",
    "\n",
    "embedding_dimension = 2 # desired embedding dimension\n",
    "\n",
    "LEN = 5000\n",
    "\n",
    "eta_array = np.linspace(0.005,0.0005,LEN+1)\n",
    "# gradually decreasing the step size allows for more finesse in later epochs\n",
    "\n",
    "for i in range(100):\n",
    "    eta_array[i] = 1.0\n",
    "# note the boolean statement in the definition of L1_array below which pairs with this;\n",
    "# we find it helpful to allow an initial period with large step size and no topological loss function (so, n2v only)\n",
    "# this arranges the initially random embedding into a more accurate shape before continuing with the full learning process\n",
    "\n",
    "lambda0 = 1 # for node2vec (should be MUCH smaller than lambda1, lambda2)\n",
    "lambda1 = 192 # for dim1 homology\n",
    "lambda2 = 0 # for dim2 homology\n",
    "\n",
    "L0_array = [lambda0 for i in range(LEN+1)]\n",
    "L1_array = [lambda1*int(i > 100) for i in range(LEN+1)]\n",
    "L2_array = [lambda2 for i in range(LEN+1)]\n",
    "\n",
    "\n",
    "\n",
    "#### GENERATE POINTCLOUD INPUT DATA\n",
    "    \n",
    "# circle number\n",
    "cn = 8\n",
    "\n",
    "# circle density\n",
    "cd = 16\n",
    "\n",
    "data = circles(cn,cd) # input data, should be a pd.DataFrame\n",
    "\n",
    "\n",
    "\n",
    "#### SETTING INPUT DATA\n",
    "\n",
    "# any non-empty subset of the following three pieces of information can be provided;\n",
    "# if distance_matrx or correlation_matrix are not assigned, they will be generated automatically.\n",
    "# while you can start from a pointcloud, no pointcloud is required to run this code.\n",
    "\n",
    "# if you are starting from a graph (as is the original purpose of Node2vec), set correlation_matrix equal to the graph's adjacency matrix\n",
    "\n",
    "data_pointcloud=data\n",
    "data_distance_matrix=None\n",
    "data_correlation_matrix=None\n",
    "\n",
    "\n",
    "\n",
    "#### NODE2VEC NBHD GENERATION PARAMETERS\n",
    "\n",
    "r = 1000 # number of walks generated from each vertex in node2vec\n",
    "l = 1000 # length of walks generated from each vertex in node2vec\n",
    "\n",
    "# if r*l > size of data set, no nbhd will be generated, and instead the full correlation vector for each node will be used\n",
    "# (https://arxiv.org/pdf/2309.08241.pdf, Remark 1)\n",
    "\n",
    "nbhd_regen = None\n",
    "\n",
    "# if generating random walks instead of using the above case, you can select how often to regenerate the nbhds;\n",
    "# set this to 1 unless you want to experiment with static neighborhoods\n",
    "\n",
    "# if nbhd_regen is None, we enter the same case as above (use full correlation vectors)\n",
    "\n",
    "L_array = [l for i in range(LEN+1)]\n",
    "R_array = [r for i in range(LEN+1)]\n",
    "P_array = [0 for i in range(LEN+1)]\n",
    "Q_array = [1 for i in range(LEN+1)]\n",
    "\n",
    "param_array = [{'l':L_array[i],\n",
    "                'r':R_array[i],\n",
    "                'p':P_array[i],\n",
    "                'q':Q_array[i]}\n",
    "               for i in range(LEN+1)]\n",
    "\n",
    "\n",
    "\n",
    "#### FURTHER ADJUSTMENT\n",
    "\n",
    "mbs_array = [int(data.shape[0]*0.25) for i in range(LEN+1)] # mini-batch size â€” input is in number of data points, adjust the multiplier to choose by percent\n",
    "\n",
    "lift_array = [0 for i in range(LEN+1)] # if you want to lift the PDs before matching, assign here; value is a multiplier on the DIAMETER of the target PD (should only ever need a value in [0.0, 1.0])\n",
    "\n",
    "target_pd = None # if you want to provide an artificial target PD, assign it here (as a pd.DataFrame)\n",
    "W1_data = None # if you want to start with a non-random initial state for W1 (the embedding), assign it here\n",
    "\n",
    "\n",
    "\n",
    "#### OUTPUT SETTINGS\n",
    "\n",
    "pointcloud_vf_save = np.arange(LEN+1,step=100) # at what epochs do you want to save .png files of the ongoing embedding with vector fields denoting gradient movement (in 2D)?\n",
    "\n",
    "pointcloud_data_save = np.arange(LEN+1,step=100) # at what epochs do you want to save .csv files of the current embedding?\n",
    "\n",
    "# it is highly recommended to print often when running a new experiment for quick feedback on hyperparameter adjustment\n",
    "\n",
    "\n",
    "\n",
    "if os.path.isdir(main_directory+project_name) and be_careful:\n",
    "    \n",
    "    print('This project directory already exists.')\n",
    "\n",
    "else:\n",
    "\n",
    "    X = tn2v.tn2v(\n",
    "        main_directory=main_directory,\n",
    "        project_name=project_name,\n",
    "        embedding_dimension=embedding_dimension,\n",
    "        n2v_param_array=param_array,\n",
    "        l0_array=L0_array,\n",
    "        l1_array=L1_array,\n",
    "        l2_array=L2_array,\n",
    "        eta_array=eta_array,\n",
    "        LEN=LEN,\n",
    "        cpu_gpu='gpu',\n",
    "        nbhd_regen=nbhd_regen,\n",
    "        data_pointcloud=data_pointcloud,\n",
    "        data_distance_matrix=data_distance_matrix,\n",
    "        data_correlation_matrix=data_correlation_matrix,\n",
    "        mbs_array=mbs_array,\n",
    "        lift_array=lift_array,\n",
    "        target_pd=target_pd,\n",
    "        initial_W1=None,\n",
    "        pointcloud_data_save=pointcloud_data_save,\n",
    "        pointcloud_vf_save=pointcloud_vf_save,\n",
    "        reciprocal_gamma=0.001,\n",
    "        reciprocal_nu=1.0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5f397f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84317fa8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0369037f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "main_directory = home+'/tn2v_output/'\n",
    "# local directory for saving output\n",
    "\n",
    "if not os.path.isdir(main_directory):\n",
    "    os.mkdir(main_directory)\n",
    "\n",
    "    \n",
    "\n",
    "project_name = 'torus_example/'\n",
    "# subfolder for this project\n",
    "\n",
    "be_careful = False\n",
    "# True: if there is already a project directory with this project_name, stop\n",
    "# False: you're running the same experiment again and just want to overwrite\n",
    "\n",
    "\n",
    "\n",
    "embedding_dimension = 3 # desired embedding dimension\n",
    "\n",
    "LEN = 40000\n",
    "\n",
    "eta_array = np.linspace(0.0025,0.000125,LEN+1)\n",
    "# gradually decreasing the step size allows for more finesse in later epochs\n",
    "\n",
    "for i in range(100):\n",
    "    eta_array[i] = 1.0\n",
    "# note the boolean statement in the definition of L1_array below which pairs with this;\n",
    "# we find it helpful to allow an initial period with large step size and no topological loss function (so, n2v only)\n",
    "# this arranges the initially random embedding into a more accurate shape before continuing with the full learning process\n",
    "\n",
    "lambda0 = 1 # for node2vec\n",
    "lambda1 = 256 # for dim1 homology\n",
    "lambda2 = 784 # for dim2 homology\n",
    "\n",
    "L0_array = [lambda0 for i in range(LEN+1)]\n",
    "L1_array = [lambda1*float(LEN/4+(3*i/4))/float(LEN)*int(i>100) for i in range(LEN+1)]\n",
    "L2_array = [lambda2*float(LEN/4+(3*i/4))/float(LEN)*int(i>100) for i in range(LEN+1)]\n",
    "# we ramp up the lambda1, lambda2 values over time just to avoid over-aggresive movements in the beginning when eta is larger\n",
    "\n",
    "\n",
    "\n",
    "#### GENERATE POINTCLOUD INPUT DATA\n",
    "    \n",
    "data = torus()\n",
    "\n",
    "\n",
    "\n",
    "#### SETTING INPUT DATA\n",
    "\n",
    "# any non-empty subset of the following three pieces of information can be provided;\n",
    "# if distance_matrx or correlation_matrix are not assigned, they will be generated automatically.\n",
    "# while you can start from a pointcloud, no pointcloud is required to run this code.\n",
    "\n",
    "# if you are starting from a graph (as is the original purpose of Node2vec), set correlation_matrix equal to the graph's adjacency matrix\n",
    "\n",
    "data_pointcloud=data\n",
    "data_distance_matrix=None\n",
    "data_correlation_matrix=None\n",
    "\n",
    "\n",
    "\n",
    "#### NODE2VEC NBHD GENERATION PARAMETERS\n",
    "\n",
    "r = 1000 # number of walks generated from each vertex in node2vec\n",
    "l = 1000 # length of walks generated from each vertex in node2vec\n",
    "\n",
    "# if r*l > size of data set, no nbhd will be generated, and instead the full correlation vector for each node will be used\n",
    "# (https://arxiv.org/pdf/2309.08241.pdf, Remark 1)\n",
    "\n",
    "nbhd_regen = None\n",
    "\n",
    "# if generating random walks instead of using the above case, you can select how often to regenerate the nbhds;\n",
    "# set this to 1 unless you want to experiment with static neighborhoods\n",
    "\n",
    "# if nbhd_regen is None, we enter the same case as above (use full correlation vectors)\n",
    "\n",
    "L_array = [l for i in range(LEN+1)]\n",
    "R_array = [r for i in range(LEN+1)]\n",
    "P_array = [0 for i in range(LEN+1)]\n",
    "Q_array = [1 for i in range(LEN+1)]\n",
    "\n",
    "param_array = [{'l':L_array[i],\n",
    "                'r':R_array[i],\n",
    "                'p':P_array[i],\n",
    "                'q':Q_array[i]}\n",
    "               for i in range(LEN+1)]\n",
    "\n",
    "\n",
    "\n",
    "#### FURTHER ADJUSTMENT\n",
    "\n",
    "mbs_array = [int(data.shape[0]*0.0625) for i in range(LEN+1)] # mini-batch size â€” input is in number of data points, adjust the multiplier to choose by percent\n",
    "\n",
    "lift_array = [0.75 for i in range(LEN+1)] # if you want to lift the PDs before matching, assign here; value is a multiplier on the DIAMETER of the target PD (should only ever need a value in [0.0, 1.0])\n",
    "\n",
    "target_pd = None # if you want to provide an artificial target PD, assign it here (as a pd.DataFrame)\n",
    "W1_data = None # if you want to start with a non-random initial state for W1 (the embedding), assign it here\n",
    "\n",
    "\n",
    "\n",
    "#### OUTPUT SETTINGS\n",
    "\n",
    "pointcloud_vf_save = np.arange(LEN+1,step=100) # at what epochs do you want to save .png files of the ongoing embedding with vector fields denoting gradient movement (in 2D)?\n",
    "\n",
    "pointcloud_data_save = np.arange(LEN+1,step=100) # at what epochs do you want to save .csv files of the current embedding?\n",
    "\n",
    "# it is highly recommended to print often when running a new experiment for quick feedback on hyperparameter adjustment\n",
    "\n",
    "\n",
    "\n",
    "if os.path.isdir(main_directory+project_name) and be_careful:\n",
    "    \n",
    "    print('This project directory already exists.')\n",
    "\n",
    "else:\n",
    "\n",
    "    X = tn2v.tn2v(\n",
    "        main_directory=main_directory,\n",
    "        project_name=project_name,\n",
    "        embedding_dimension=embedding_dimension,\n",
    "        n2v_param_array=param_array,\n",
    "        l0_array=L0_array,\n",
    "        l1_array=L1_array,\n",
    "        l2_array=L2_array,\n",
    "        eta_array=eta_array,\n",
    "        LEN=LEN,\n",
    "        cpu_gpu='gpu',\n",
    "        nbhd_regen=nbhd_regen,\n",
    "        data_pointcloud=data_pointcloud,\n",
    "        data_distance_matrix=data_distance_matrix,\n",
    "        data_correlation_matrix=data_correlation_matrix,\n",
    "        mbs_array=mbs_array,\n",
    "        lift_array=lift_array,\n",
    "        target_pd=target_pd,\n",
    "        initial_W1=None,\n",
    "        pointcloud_data_save=pointcloud_data_save,\n",
    "        pointcloud_vf_save=pointcloud_vf_save,\n",
    "        reciprocal_gamma=0.001,\n",
    "        reciprocal_nu=0.8 # the torus has a tendency to show extreme saddle distortion in the n2v loss function, likely to do the hyperbolic reciprocal process for making the correlation matrix; we lower the exponent slightly to mitigate this\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac9bc94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
